{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ed00e1",
   "metadata": {},
   "source": [
    "# Batch  processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78df3a",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192f53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"SPARK_API\")\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880ffb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Pclass|count|\n",
      "+------+-----+\n",
      "|     1|  216|\n",
      "|     3|  491|\n",
      "|     2|  184|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of passengers in each class in the Titanic dataset:\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "passengerCounts = df.groupBy(\"Pclass\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "passengerCounts.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819855a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|Pclass|0-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|\n",
      "+------+----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|     1|   3|   18|   40|   49|   37|   25|   11|    3|\n",
      "|     3|  44|   79|  128|   61|   28|    5|    3|    1|\n",
      "|     2|  17|   18|   61|   43|   19|   12|    3|    0|\n",
      "+------+----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of passengers by age group (e.g. 0-10 years, 11-20 years, etc.) and by class:\n",
    "\n",
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "ageRanges = [(0, 10), (11, 20), (21, 30), (31, 40), (41, 50), (51, 60), (61, 70), (71, 80)]\n",
    "\n",
    "ageRangesDf = df\n",
    "for lower, upper in ageRanges:\n",
    "    ageRangesDf = ageRangesDf.withColumn(f\"AgeRange_{lower}_to_{upper}\", when(df.Age.between(lower, upper), 1).otherwise(0))\n",
    "\n",
    "passengerCountsByAge = ageRangesDf.groupBy(\"Pclass\") \\\n",
    "                                 .agg(count(when(ageRangesDf.AgeRange_0_to_10 == 1, True)).alias(\"0-10\"),\n",
    "                                      count(when(ageRangesDf.AgeRange_11_to_20 == 1, True)).alias(\"11-20\"),\n",
    "                                      count(when(ageRangesDf.AgeRange_21_to_30 == 1, True)).alias(\"21-30\"),\n",
    "                                      count(when(ageRangesDf.AgeRange_31_to_40 == 1, True)).alias(\"31-40\"),\n",
    "                                      count(when(ageRangesDf.AgeRange_41_to_50 == 1, True)).alias(\"41-50\"),\n",
    "                                      count(when(ageRangesDf.AgeRange_51_to_60 == 1, True)).alias(\"51-60\"),\n",
    "                                      count(when(ageRangesDf.AgeRange_61_to_70 == 1, True)).alias(\"61-70\"),\n",
    "                                      count(when(ageRangesDf.AgeRange_71_to_80 == 1, True)).alias(\"71-80\"))\n",
    "\n",
    "passengerCountsByAge.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a42398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|Pclass|   Sex|count|\n",
      "+------+------+-----+\n",
      "|     2|female|   76|\n",
      "|     3|  male|  347|\n",
      "|     1|  male|  122|\n",
      "|     3|female|  144|\n",
      "|     1|female|   94|\n",
      "|     2|  male|  108|\n",
      "+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of passengers by gender in each class\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "passengerCountsBySex = df.groupBy(\"Pclass\", \"Sex\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "passengerCountsBySex.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5def8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determining the number of passengers between survivors and non-survivors\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "survivorsCounts = df.groupBy(\"Survived\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "survivorsCounts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02fcc491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+\n",
      "|Pclass|   Sex|           avg_age|\n",
      "+------+------+------------------+\n",
      "|     2|female|28.722972972972972|\n",
      "|     3|  male|26.507588932806325|\n",
      "|     1|  male| 41.28138613861386|\n",
      "|     3|female|             21.75|\n",
      "|     1|female| 34.61176470588235|\n",
      "|     2|  male| 30.74070707070707|\n",
      "+------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average age of passengers by gender and by class:\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "avgAgeBySexAndClass = df.groupBy(\"Pclass\", \"Sex\").agg(avg(\"Age\").alias(\"avg_age\"))\n",
    "\n",
    "avgAgeBySexAndClass.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30ad964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|Survived|           avg_age|\n",
      "+--------+------------------+\n",
      "|       1|28.343689655172415|\n",
      "|       0| 30.62617924528302|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average age of passengers by survival\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "avgAgeBySurvival = df.groupBy(\"Survived\").agg(avg(\"Age\").alias(\"avg_age\"))\n",
    "\n",
    "avgAgeBySurvival.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214224dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           avg_age|\n",
      "+------------------+\n",
      "|38.233440860215055|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average age of first class passengers\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "avgAgeFirstClass = df.filter(df.Pclass == 1).agg(avg(\"Age\").alias(\"avg_age\"))\n",
    "\n",
    "avgAgeFirstClass.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b4e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+\n",
      "|Pclass|Survived|           avg_age|\n",
      "+------+--------+------------------+\n",
      "|     1|       0|        43.6953125|\n",
      "|     3|       1|20.646117647058823|\n",
      "|     1|       1| 35.36819672131148|\n",
      "|     2|       1| 25.90156626506024|\n",
      "|     2|       0|33.544444444444444|\n",
      "|     3|       0|26.555555555555557|\n",
      "+------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average age of passengers by class and survival status\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "avgAgeByClassAndSurvival = df.groupBy(\"Pclass\", \"Survived\").agg(avg(\"Age\").alias(\"avg_age\"))\n",
    "\n",
    "avgAgeByClassAndSurvival.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba2275",
   "metadata": {},
   "source": [
    "## Exercises to solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca3fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|min(Age)|max(Age)|\n",
      "+--------+--------+\n",
      "|    0.42|    80.0|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the age of the oldest and youngest passengers.\n",
    "from pyspark.sql.functions import avg, min, max\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "age_aggregation = df.agg(min(\"Age\"),max(\"Age\"))\n",
    "\n",
    "age_aggregation.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573a993",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+--------+--------+  \n",
    "|min(Age)|max(Age)|  \n",
    "+--------+--------+  \n",
    "|    0.42|    80.0|  \n",
    "+--------+--------+    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b4c9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First class, number of women over 30: 50\n",
      "First class, age 30 and over: 75\n"
     ]
    }
   ],
   "source": [
    "# Determining the number of women and men aged 30 and over in first class\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "females_cnt = df.filter(\"Pclass = 1 and Age > 30 and Sex == 'female'\" ).count()\n",
    "males_cnt = df.filter(\"Pclass = 1 and Age > 30 and Sex == 'male'\" ).count()\n",
    "\n",
    "print(f'First class, number of women over 30: {females_cnt}')\n",
    "print(f'First class, age 30 and over: {males_cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb78645",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "First class, number of women over 30: 50\n",
    "First class, age 30 and over: 75\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46f381f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|  Cabin|      SurvivalRate|\n",
      "+-------+------------------+\n",
      "|    A23|               1.0|\n",
      "|    B79|               1.0|\n",
      "|    E44|               0.5|\n",
      "|  F E69|               1.0|\n",
      "|    D28|               1.0|\n",
      "|    C78|               0.5|\n",
      "|    C95|               0.0|\n",
      "|  F G73|               0.0|\n",
      "|B58 B60|               0.5|\n",
      "|     D7|               1.0|\n",
      "|   C128|               0.0|\n",
      "|    B39|               1.0|\n",
      "|    B22|               0.5|\n",
      "|   C110|               0.0|\n",
      "|    D21|               1.0|\n",
      "|     F2|0.6666666666666666|\n",
      "|    B30|               0.0|\n",
      "|   C104|               1.0|\n",
      "|    B50|               1.0|\n",
      "|     A6|               1.0|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passenger survival rates by cabin.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "cabin_survivors = df.groupBy(\"Cabin\").agg(avg(\"Survived\").alias(\"SurvivalRate\"))\n",
    "\n",
    "cabin_survivors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4b336",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+-------+------------------+\n",
    "|  Cabin|      SurvivalRate|\n",
    "+-------+------------------+\n",
    "|    A23|               1.0|\n",
    "|    B79|               1.0|\n",
    "|    E44|               0.5|\n",
    "|  F E69|               1.0|\n",
    "|    D28|               1.0|\n",
    "|    C78|               0.5|\n",
    "|    C95|               0.0|\n",
    "|  F G73|               0.0|\n",
    "|B58 B60|               0.5|\n",
    "|     D7|               1.0|\n",
    "|   C128|               0.0|\n",
    "|    B39|               1.0|\n",
    "|    B22|               0.5|\n",
    "|   C110|               0.0|\n",
    "|    D21|               1.0|\n",
    "|     F2|0.6666666666666666|\n",
    "|    B30|               0.0|\n",
    "|   C104|               1.0|\n",
    "|    B50|               1.0|\n",
    "|     A6|               1.0|\n",
    "+-------+------------------+\n",
    "only showing top 20 rows\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d66b814e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|Embarked|       SurvivalRate|\n",
      "+--------+-------------------+\n",
      "|       Q|0.38961038961038963|\n",
      "|    null|                1.0|\n",
      "|       C| 0.5535714285714286|\n",
      "|       S|0.33695652173913043|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passenger survival based on the place of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "from pyspark.sql.functions import avg\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "embarked_survivors = df.groupBy(\"Embarked\").agg(avg(\"Survived\") .alias(\"SurvivalRate\"))\n",
    "embarked_survivors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e022a",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+--------+-------------------+\n",
    "|Embarked|       SurvivalRate|\n",
    "+--------+-------------------+\n",
    "|       Q|0.38961038961038963|\n",
    "|    null|                1.0|\n",
    "|       C| 0.5535714285714286|\n",
    "|       S|0.33695652173913043|\n",
    "+--------+-------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa08edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|rounded_age|count|\n",
      "+-----------+-----+\n",
      "|       30.0|  201|\n",
      "|       20.0|  200|\n",
      "|       null|  177|\n",
      "|       40.0|  120|\n",
      "|       50.0|   73|\n",
      "|        0.0|   40|\n",
      "|       10.0|   38|\n",
      "|       60.0|   31|\n",
      "|       70.0|   10|\n",
      "|       80.0|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The most common age group among passengers is 10 years.Scroll down. 0, 10, 20, etc..\n",
    "from pyspark.sql.functions import round, expr, count\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "rounded_age = df.withColumn('rounded_age', expr('round(age, -1)')).groupBy('rounded_age').agg(count('*').alias('count')).orderBy(\"count\", ascending=False)\n",
    "\n",
    "rounded_age.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e005b",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+-----------+-----+\n",
    "|Age_rounded|count|\n",
    "+-----------+-----+\n",
    "|       30.0|  201|\n",
    "|       20.0|  200|\n",
    "|       40.0|  120|\n",
    "|       50.0|   73|\n",
    "|        0.0|   40|\n",
    "|       10.0|   38|\n",
    "|       60.0|   31|\n",
    "|       70.0|   10|\n",
    "|       80.0|    1|\n",
    "|       null|    0|\n",
    "+-----------+-----+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b379067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------------+\n",
      "|rounded_age|count|      avg(Survived)|\n",
      "+-----------+-----+-------------------+\n",
      "|       30.0|  201| 0.3880597014925373|\n",
      "|       20.0|  200|              0.365|\n",
      "|       null|  177| 0.2937853107344633|\n",
      "|       40.0|  120|              0.425|\n",
      "|       50.0|   73|  0.410958904109589|\n",
      "|        0.0|   40|              0.675|\n",
      "|       10.0|   38|0.47368421052631576|\n",
      "|       60.0|   31| 0.3870967741935484|\n",
      "|       70.0|   10|                0.0|\n",
      "|       80.0|    1|                1.0|\n",
      "+-----------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The survival rate of passengers in the most common age group.\n",
    "from pyspark.sql.functions import avg, round, expr, count\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "rounded_age_survival = df.withColumn('rounded_age', expr('round(age, -1)')).groupBy('rounded_age').agg(count('*').alias('count'), avg('Survived')).orderBy(\"count\", ascending=False)\n",
    "\n",
    "rounded_age_survival.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26daae",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+-----------+-----+-------------------+\n",
    "|Age_rounded|count|        AvgSurvived|\n",
    "+-----------+-----+-------------------+\n",
    "|       30.0|  201| 0.3880597014925373|\n",
    "|       20.0|  200|              0.365|\n",
    "|       40.0|  120|              0.425|\n",
    "|       50.0|   73|  0.410958904109589|\n",
    "|        0.0|   40|              0.675|\n",
    "|       10.0|   38|0.47368421052631576|\n",
    "|       60.0|   31| 0.3870967741935484|\n",
    "|       70.0|   10|                0.0|\n",
    "|       80.0|    1|                1.0|\n",
    "|       null|    0| 0.2937853107344633|\n",
    "+-----------+-----+-------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc72abca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "|Survived|          avg(Age)|       avg(Pclass)|\n",
      "+--------+------------------+------------------+\n",
      "|       1|28.343689655172415|1.9502923976608186|\n",
      "|       0| 30.62617924528302|2.5318761384335153|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average age and class of passengers by survival.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "survival_age = df.groupBy(\"Survived\").agg(avg(\"Age\"), avg(\"Pclass\"))\n",
    "\n",
    "survival_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea034f2",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+--------+------------------+------------------+\n",
    "|Survived|          avg(Age)|       avg(Pclass)|\n",
    "+--------+------------------+------------------+\n",
    "|       1|28.343689655172415|1.9502923976608186|\n",
    "|       0| 30.62617924528302|2.5318761384335153|\n",
    "+--------+------------------+------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0941ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+\n",
      "|Pclass|   Sex|count(PassengerId)|\n",
      "+------+------+------------------+\n",
      "|     2|female|                76|\n",
      "|     3|  male|               347|\n",
      "|     1|  male|               122|\n",
      "|     3|female|               144|\n",
      "|     1|female|                94|\n",
      "|     2|  male|               108|\n",
      "+------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total number of passengers, by class and gender.\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "class_sex = df.groupBy(\"Pclass\", \"Sex\").agg(count(\"PassengerId\"))\n",
    "\n",
    "class_sex.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8a6c8",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+------+------+------------------+\n",
    "|Pclass|   Sex|count(PassengerId)|\n",
    "+------+------+------------------+\n",
    "|     2|female|                76|\n",
    "|     3|  male|               347|\n",
    "|     1|  male|               122|\n",
    "|     3|female|               144|\n",
    "|     1|female|                94|\n",
    "|     2|  male|               108|\n",
    "+------+------+------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b21f4795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|      avg(Survived)|\n",
      "+-------------------+\n",
      "|0.29985443959243085|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the survival rate among passengers whose cabin is unknown.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "no_cabin = df.filter(\"Cabin is NULL\").agg(avg(\"Survived\"))\n",
    "\n",
    "no_cabin.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fbf0a",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+-------------------+\n",
    "|      avg(Survived)|\n",
    "+-------------------+\n",
    "|0.29985443959243085|\n",
    "+-------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a4ffc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+\n",
      "|Pclass|Survived|count(PassengerId)|\n",
      "+------+--------+------------------+\n",
      "|     1|       0|                21|\n",
      "|     3|       1|               113|\n",
      "|     1|       1|                19|\n",
      "|     2|       1|                74|\n",
      "|     2|       0|                94|\n",
      "|     3|       0|               366|\n",
      "+------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total number of passengers with no known cabin, by class and survival status.\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"titanic.csv\")\n",
    "\n",
    "class_survived = df.filter(\"Cabin is NULL\").groupBy(\"Pclass\", \"Survived\").agg(count(\"PassengerId\"))\n",
    "\n",
    "class_survived.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe0df4",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "+------+--------+------------------+\n",
    "|PClass|Survived|count(PassengerId)|\n",
    "+------+--------+------------------+\n",
    "|     1|       0|                21|\n",
    "|     3|       1|               113|\n",
    "|     1|       1|                19|\n",
    "|     2|       1|                74|\n",
    "|     2|       0|                94|\n",
    "|     3|       0|               366|\n",
    "+------+--------+------------------+\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
